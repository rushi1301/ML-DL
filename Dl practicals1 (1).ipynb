{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b50b3a1-0c23-404c-a878-586b812df9f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Convo2D' from 'tensorflow.keras.layers' (C:\\Users\\AADITYA MANE\\anaconda3\\Lib\\site-packages\\keras\\_tf_keras\\keras\\layers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Convo2D,Maxpooling2D,Flatten,Dense\n\u001b[0;32m      8\u001b[0m (trainx,trainy),(testx,testy)\u001b[38;5;241m=\u001b[39mmnist\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m      9\u001b[0m trainx\u001b[38;5;241m=\u001b[39mtrainx\u001b[38;5;241m.\u001b[39mr\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Convo2D' from 'tensorflow.keras.layers' (C:\\Users\\AADITYA MANE\\anaconda3\\Lib\\site-packages\\keras\\_tf_keras\\keras\\layers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#CNN on mnist data\n",
    "import numpy as np \n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convo2D,Maxpooling2D,Flatten,Dense\n",
    "\n",
    "(trainx,trainy),(testx,testy)=mnist.load_data()\n",
    "trainx=trainx.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc1dfd63-2638-425d-9632-12c728c39d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AADITYA MANE\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 35ms/step - accuracy: 0.3777 - loss: 1.7093 - val_accuracy: 0.5636 - val_loss: 1.2327\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.5849 - loss: 1.1697 - val_accuracy: 0.6199 - val_loss: 1.0651\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 29ms/step - accuracy: 0.6501 - loss: 1.0044 - val_accuracy: 0.6599 - val_loss: 0.9849\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.6902 - loss: 0.8976 - val_accuracy: 0.6792 - val_loss: 0.9346\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.7161 - loss: 0.8144 - val_accuracy: 0.6826 - val_loss: 0.9178\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 29ms/step - accuracy: 0.7511 - loss: 0.7232 - val_accuracy: 0.6948 - val_loss: 0.9017\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.7730 - loss: 0.6599 - val_accuracy: 0.7131 - val_loss: 0.8570\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 29ms/step - accuracy: 0.7941 - loss: 0.5978 - val_accuracy: 0.7036 - val_loss: 0.8949\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.8115 - loss: 0.5392 - val_accuracy: 0.7168 - val_loss: 0.8787\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 29ms/step - accuracy: 0.8344 - loss: 0.4784 - val_accuracy: 0.6890 - val_loss: 1.0089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17464c72720>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CNN on cifar \n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "# Load and preprocess data\n",
    "(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "trainX = trainX.astype('float32') / 255.0\n",
    "testX = testX.astype('float32') / 255.0\n",
    "trainY = to_categorical(trainY)\n",
    "testY = to_categorical(testY)\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=64, validation_data=(testX, testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d555903-0958-4b74-a4d4-287326ecf6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reinforcement learning maze problem.\n",
    "import numpy as np\n",
    "\n",
    "# Maze: 0-free, 1-wall, 2-start, 3-goal\n",
    "maze = np.array([\n",
    "    [2, 0, 1, 0, 3],\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "n_states = maze.size\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "Q = np.zeros((n_states, len(actions)))\n",
    "\n",
    "def state_to_pos(state):\n",
    "    return divmod(state, maze.shape[1])\n",
    "\n",
    "def pos_to_state(pos):\n",
    "    return pos[0] * maze.shape[1] + pos[1]\n",
    "\n",
    "def valid_actions(pos):\n",
    "    acts = []\n",
    "    for i, (dx, dy) in enumerate([(-1,0),(1,0),(0,-1),(0,1)]):\n",
    "        nx, ny = pos[0]+dx, pos[1]+dy\n",
    "        if 0 <= nx < maze.shape[0] and 0 <= ny < maze.shape[1] and maze[nx, ny] != 1:\n",
    "            acts.append(i)\n",
    "    return acts\n",
    "\n",
    "# Q-learning\n",
    "alpha, gamma, epsilon, episodes = 0.8, 0.95, 0.1, 500\n",
    "for ep in range(episodes):\n",
    "    state = np.where(maze.flatten() == 2)[0][0]\n",
    "    while True:\n",
    "        pos = state_to_pos(state)\n",
    "        acts = valid_actions(pos)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(acts)\n",
    "        else:\n",
    "            action = acts[np.argmax(Q[state, acts])]\n",
    "        dx, dy = [(-1,0),(1,0),(0,-1),(0,1)][action]\n",
    "        nx, ny = pos[0]+dx, pos[1]+dy\n",
    "        next_state = pos_to_state((nx, ny))\n",
    "        reward = 1 if maze[nx, ny] == 3 else -0.04\n",
    "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        state = next_state\n",
    "        if maze[nx, ny] == 3:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19bfd76e-f993-4bcb-ba6a-52a726b4c724",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NSE-TATA.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNSE-TATA.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NSE-TATA.csv'"
     ]
    }
   ],
   "source": [
    "#Sequence learning for stock price prediction.(LSTM).\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('NSE-TATA.csv')\n",
    "data = df['Close'].values.reshape(-1,1)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare sequences\n",
    "X, y = [], []\n",
    "for i in range(60, len(scaled_data)):\n",
    "    X.append(scaled_data[i-60:i, 0])\n",
    "    y.append(scaled_data[i, 0])\n",
    "X, y = np.array(X), np.array(y)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Build LSTM\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "    LSTM(50),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X, y, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fab627-2f7d-4b36-b85e-146e9a34326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 .Recommendation system.\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "movies = pd.read_csv('movies.csv')\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "\n",
    "# Merge\n",
    "movie_ratings = pd.merge(ratings, movies, on='movieId')\n",
    "\n",
    "# Create pivot table\n",
    "user_movie_ratings = movie_ratings.pivot_table(index='userId', columns='title', values='rating')\n",
    "\n",
    "# Function to recommend movies\n",
    "def recommend(movie_name):\n",
    "    movie_corr = user_movie_ratings.corrwith(user_movie_ratings[movie_name])\n",
    "    recommendations = pd.DataFrame(movie_corr, columns=['Correlation'])\n",
    "    recommendations = recommendations.join(movies.set_index('title'))\n",
    "    recommendations = recommendations[recommendations['Correlation'].notnull()]\n",
    "    recommendations = recommendations.sort_values('Correlation', ascending=False)\n",
    "    return recommendations.head(5)\n",
    "\n",
    "print(recommend('Toy Story (1995)'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba410fe-f0f7-41a0-b268-e8cd3395e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 GAN handwritten images \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5  # Scale to [-1, 1]\n",
    "X_train = X_train.reshape(-1, 28*28)\n",
    "\n",
    "# Generator model\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=100))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(28*28, activation='tanh'))\n",
    "    model.add(Reshape((28, 28, 1)))\n",
    "    return model\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28,28,1)))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Build and compile discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Build generator\n",
    "generator = build_generator()\n",
    "\n",
    "# GAN model (stacked generator and discriminator)\n",
    "discriminator.trainable = False\n",
    "gan = Sequential([generator, discriminator])\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10000\n",
    "batch_size = 128\n",
    "sample_interval = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Train discriminator\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    real_imgs = X_train[idx].reshape(batch_size, 28, 28, 1)\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    fake_imgs = generator.predict(noise)\n",
    "    real_y = np.ones((batch_size, 1))\n",
    "    fake_y = np.zeros((batch_size, 1))\n",
    "    d_loss_real = discriminator.train_on_batch(real_imgs, real_y)\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_y)\n",
    "    \n",
    "    # Train generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    valid_y = np.ones((batch_size, 1))\n",
    "    g_loss = gan.train_on_batch(noise, valid_y)\n",
    "    \n",
    "    # Print progress and sample images\n",
    "    if (epoch + 1) % sample_interval == 0:\n",
    "        print(f\"{epoch+1} [D loss: {0.5 * np.add(d_loss_real, d_loss_fake)[0]:.4f}] [G loss: {g_loss:.4f}]\")\n",
    "        # Generate and plot sample images\n",
    "        noise = np.random.normal(0, 1, (25, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5  # Rescale to [0,1]\n",
    "        fig, axs = plt.subplots(5, 5)\n",
    "        cnt = 0\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
